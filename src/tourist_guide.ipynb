{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6c3afb",
   "metadata": {},
   "source": [
    "## Dependencies installation\n",
    "\n",
    "We install the main dependencies that will be used alongside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d38c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27458f64",
   "metadata": {},
   "source": [
    "## Load the Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8571e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# cargamos las variables/claves desde el .env\n",
    "dotenv_loaded = load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Google API Key was not set properly, please share it here: \")\n",
    "    \n",
    "# comprobamos que se han cargado correctamente\n",
    "if os.environ[\"GOOGLE_API_KEY\"]==\"\":\n",
    "    print(\"'GOOGLE_API_KEY' wasn't set correctly. Please make sure the keys/variables are accesible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b3e98",
   "metadata": {},
   "source": [
    "Let's make a little trial to ensure the API key is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b736c64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El máximo goleador vasco de LaLiga en la temporada 2020-2021 fue **Mikel Oyarzabal**, de la Real Sociedad, con **11 goles**.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Dime quien es el máximo goleador vasco de LaLiga en la temporada 2020-2021\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a832ec",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ef5c6",
   "metadata": {},
   "source": [
    "We will load the cities data so we can work on it and later divide it in chunks so it's more manageable. First we will code a function to help us divide each of the documents in pages before applying the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8becab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "def document_reader(path, doc_name):\n",
    "    \n",
    "    # Abrimos el archivo para leerlo de forma binaria\n",
    "    doc_path = path + doc_name\n",
    "    pdf_reader = PdfReader(doc_path)\n",
    "    \n",
    "    global_text = []\n",
    "    for i, page in enumerate(pdf_reader.pages, start=1):\n",
    "        text = page.extract_text()\n",
    "        # global_text[pdf_reader.pages[i].extract_text()] = {f\"Page {i+1}\": f\"{doc_name}\"}\n",
    "        \n",
    "        global_text.append({\n",
    "            \"page_content\": text.strip(), # limpia espacios sobrentes y los saltos de linea\n",
    "            \"doc_ubication\": { \"document\": doc_name, \"page\": i }\n",
    "        })\n",
    "        \n",
    "    return global_text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c549d",
   "metadata": {},
   "source": [
    "Now, by using function above we will create a list mixing everything in an only list so we have all the chunks/pages together and can operate easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776d34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 {'document': 'BARCELONA.pdf', 'page': 1}\n",
      "{'document': 'VALENCIA.pdf', 'page': 16}\n"
     ]
    }
   ],
   "source": [
    "def load_pdfs(data_dir=\"../data/\"):\n",
    "    documents = []\n",
    "    for file in os.listdir(\"../data/\"): # recorremos la lista de archivos en el directorio y aplicamos document_reader a cada uno de ellos\n",
    "        docs = document_reader(data_dir, file)\n",
    "        documents.extend(docs)\n",
    "    return documents\n",
    "\n",
    "documents = load_pdfs(\"../data/\")\n",
    "print(len(documents), documents[0][\"doc_ubication\"])\n",
    "print(documents[-1][\"doc_ubication\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab4dd9",
   "metadata": {},
   "source": [
    "Now we will split up everything on chunks. Each document will be having around of 40.000 characters what is a extremely large quantity if we take the whole sum of characters for every document on the data folder. Furthermore, it is not very convenient for adding them to the context window of some models, it may be difficult for these models to find the information in excessively long inputs (not to mention the increased cost of each request to the model...). \n",
    "\n",
    "That's why we will use `RecursiveCharacterTextSplitter` to divide the format following a recursive strategy in the chunk_size we decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70d71136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total splits: 223\n",
      "First split content:\n",
      "page_content='www.spain.infoBarcelona' metadata={'document': 'BARCELONA.pdf', 'page': 1, 'start_index': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "docs = [Document(page_content=d[\"page_content\"], metadata=d[\"doc_ubication\"]) for d in documents]\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Total splits: {len(all_splits)}\")\n",
    "\n",
    "# Mostramos el primer split.\n",
    "print(f\"First split content:\\n{all_splits[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685f599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
